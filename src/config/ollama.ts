// Ollama API configuration â€” switch via .env.local only; do not edit this file.
// Local:  VITE_OLLAMA_USE_CLOUD=false (or unset). Run `ollama serve` + `ollama pull phi3`. VITE_OLLAMA_BASE_URL is ignored.
// Cloud:  VITE_OLLAMA_USE_CLOUD=true, VITE_OLLAMA_API_KEY=your_key
// Vercel: VITE_OLLAMA_USE_CLOUD=true, VITE_OLLAMA_USE_PROXY=true, OLLAMA_API_KEY=<key>

const useCloud = import.meta.env.VITE_OLLAMA_USE_CLOUD === 'true'
const serverAddsAuth = import.meta.env.VITE_OLLAMA_USE_PROXY === 'true'

// Local: always /api/ollama-local (Vite proxies to localhost:11434). VITE_OLLAMA_BASE_URL is ignored for local.
function getBaseUrl(): string {
  if (useCloud) return import.meta.env.VITE_OLLAMA_BASE_URL || '/api/ollama'
  return '/api/ollama-local'
}

export const OLLAMA_CONFIG = {
  useCloud,
  serverAddsAuth,
  apiKey: import.meta.env.VITE_OLLAMA_API_KEY || '',
  baseUrl: getBaseUrl(),
  model: import.meta.env.VITE_OLLAMA_MODEL || (useCloud ? 'gpt-oss:20b-cloud' : 'phi3:latest'),
}
